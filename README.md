# LLMBench

*[English](#english-version) | [ä¸­æ–‡](#chinese-version)*

---

<a id="english-version"></a>
## English Version

AIBench is a powerful AI model evaluation framework designed to compare and test the conversational capabilities of different large language model APIs. Through standardized test scenarios and a precise scoring system, AIBench helps you objectively evaluate the performance of various AI models.

### âœ¨ Key Features

- **Multi-API Support**: Test and compare multiple AI APIs simultaneously (Monica, OpenAI, Gemini, etc.)
- **Multi-turn Conversation Evaluation**: Test AI performance in complex conversation flows
- **Comprehensive Scoring System**: Score based on relevance, accuracy, completeness, coherence, and creativity
- **Flexible Model Configuration**: Easily configure the models used by each API through environment variables
- **Rich Test Scenarios**: Preset various test scenarios covering basic Q&A, creative generation, logical reasoning, etc.
- **Detailed Evaluation Reports**: Generate comprehensive evaluation reports for intuitive comparison of different API performance

### ğŸš€ Quick Start

#### Installation

```bash
# Clone the repository
git clone https://github.com/Gary-666/LLMBench.git
cd LLMBench

# Install dependencies
pip install -r requirements.txt
```

#### Configuration

Create a `.env` file and add your API keys and model configurations:

```
# API Keys
MONICA_KEY=your_monica_api_key
OPENAI_API_KEY=your_openai_api_key
GEMINI_KEY=your_gemini_api_key

# Model Configurations
MONICA_MODEL=gpt-4o
OPENAI_TEXT_MODEL=gpt-4.1
OPENAI_VISION_MODEL=gpt-4.1
GEMINI_TEXT_MODEL=gemini-2.5-pro
GEMINI_VISION_MODEL=gemini-2.5-pro

# Proxy Configuration (Optional)
HTTP_PROXY=http://your-proxy:port
HTTPS_PROXY=http://your-proxy:port
```

### ğŸ“Š Usage

#### Interactive Conversation

Have an interactive conversation with a specific API:

```bash
python main_new.py interactive --api monica
```

#### Multi-turn Conversation Testing

Test all APIs using predefined conversation flows:

```bash
python main_new.py multi-turn --file test_scenarios/comprehensive_test.json
```

#### Compare API Responses

Compare responses from different APIs to the same prompt:

```bash
python main_new.py compare --prompt "Explain the basic principles of quantum computing"
```

#### Batch Testing

Run a series of test cases:

```bash
python main_new.py batch --file test_scenarios/basic_test.json
```

### ğŸ“ Project Structure

```
LLMBench/
â”œâ”€â”€ aims/                  # Core code directory
â”‚   â”œâ”€â”€ cli.py             # Command-line interface
â”‚   â”œâ”€â”€ clients.py         # API client implementations
â”‚   â”œâ”€â”€ evaluation.py      # Evaluation system
â”‚   â”œâ”€â”€ models.py          # Data models
â”‚   â””â”€â”€ testing.py         # Testing tools
â”œâ”€â”€ conversations/         # Conversation history storage
â”œâ”€â”€ evaluations/           # Evaluation results storage
â”œâ”€â”€ results/               # Test results storage
â”œâ”€â”€ test_scenarios/        # Test scenario JSON files
â””â”€â”€ main_new.py            # Main program entry
```

### ğŸ” Evaluation Dimensions

AIBench evaluates AI response quality using the following dimensions:

| Dimension | Description | Weight |
|-----------|-------------|--------|
| Relevance | How relevant the answer is to the question | 25% |
| Accuracy | The accuracy and correctness of the answer | 25% |
| Completeness | The comprehensiveness and thoroughness of the answer | 20% |
| Coherence | The logic and fluency of the answer | 15% |
| Creativity | The innovation and uniqueness of the answer | 15% |

### ğŸ§ª Test Scenarios

AIBench provides various preset test scenarios, including:

- **Basic Q&A**: Test basic knowledge and answering abilities
- **Context Memory**: Test the ability to remember context information
- **Chinese Language Understanding**: Test understanding of Chinese language and culture
- **Logical Reasoning**: Test logical reasoning and problem-solving abilities
- **Creative Generation**: Test creativity and imagination
- **Multi-turn Instructions**: Test the ability to execute complex instructions

### ğŸ“ˆ Custom Testing

Creating custom test scenarios is simple. Just create a JSON file in the following format:

```json
[
  {
    "name": "Test Name",
    "description": "Test Description",
    "turns": [
      {"prompt": "First round question"},
      {"prompt": "Second round question"}
    ]
  }
]
```

### ğŸ“ Command Line Arguments

| Parameter | Short | Description |
|-----------|-------|-------------|
| `mode` | | Run mode: `interactive`, `compare`, `batch`, `multi-turn` |
| `--api` | `-a` | Specify API: `monica`, `openai`, `gemini`, `all` |
| `--prompt` | `-p` | Text prompt |
| `--image` | `-i` | Image URL |
| `--file` | `-f` | Test scenario JSON file |
| `--proxy` | | Set proxy |

### ğŸ“‹ Evaluation Report Example

```
=== Overall API Scores ===
Monica API: 0.87 (Best)
OpenAI API: 0.85
Gemini API: 0.82

=== Dimension Breakdown ===
Relevance: Monica (0.92), OpenAI (0.89), Gemini (0.85)
Accuracy: OpenAI (0.90), Monica (0.88), Gemini (0.86)
Completeness: Monica (0.89), OpenAI (0.87), Gemini (0.82)
Coherence: Monica (0.85), OpenAI (0.84), Gemini (0.80)
Creativity: OpenAI (0.75), Monica (0.74), Gemini (0.70)
```

### ğŸ¤ Contributing

Contributions of code, issue reports, or improvement suggestions are welcome! Please check the [contribution guidelines](CONTRIBUTING.md) for more information.

### ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

### ğŸ“ Contact

For any questions or suggestions, please contact us through:

- Email: your.email@example.com
- GitHub: [https://github.com/Gary-666/LLMBench](https://github.com/Gary-666/LLMBench)

---

<a id="chinese-version"></a>
## ä¸­æ–‡ç‰ˆæœ¬

AIBenchæ˜¯ä¸€ä¸ªå¼ºå¤§çš„AIæ¨¡å‹è¯„ä¼°æ¡†æ¶ï¼Œä¸“ä¸ºæ¯”è¾ƒå’Œæµ‹è¯•ä¸åŒå¤§è¯­è¨€æ¨¡å‹APIçš„å¯¹è¯èƒ½åŠ›è€Œè®¾è®¡ã€‚é€šè¿‡æ ‡å‡†åŒ–çš„æµ‹è¯•åœºæ™¯å’Œç²¾ç¡®çš„è¯„åˆ†ç³»ç»Ÿï¼ŒAIBenchå¸®åŠ©æ‚¨å®¢è§‚è¯„ä¼°å„ç§AIæ¨¡å‹çš„æ€§èƒ½è¡¨ç°ã€‚

### âœ¨ ä¸»è¦ç‰¹æ€§

- **å¤šAPIæ”¯æŒ**: åŒæ—¶æµ‹è¯•å’Œæ¯”è¾ƒå¤šä¸ªAI APIï¼ˆMonicaã€OpenAIã€Geminiç­‰ï¼‰
- **å¤šè½®å¯¹è¯è¯„ä¼°**: æµ‹è¯•AIåœ¨å¤æ‚å¯¹è¯æµç¨‹ä¸­çš„è¡¨ç°
- **å…¨é¢è¯„åˆ†ç³»ç»Ÿ**: åŸºäºç›¸å…³æ€§ã€å‡†ç¡®æ€§ã€å®Œæ•´æ€§ã€è¿è´¯æ€§å’Œåˆ›æ–°æ€§è¿›è¡Œè¯„åˆ†
- **çµæ´»æ¨¡å‹é…ç½®**: é€šè¿‡ç¯å¢ƒå˜é‡è½»æ¾é…ç½®æ¯ä¸ªAPIä½¿ç”¨çš„æ¨¡å‹
- **ä¸°å¯Œæµ‹è¯•åœºæ™¯**: é¢„è®¾å¤šç§æµ‹è¯•åœºæ™¯ï¼Œè¦†ç›–åŸºç¡€é—®ç­”ã€åˆ›æ„ç”Ÿæˆã€é€»è¾‘æ¨ç†ç­‰
- **è¯¦ç»†è¯„ä¼°æŠ¥å‘Š**: ç”Ÿæˆå…¨é¢çš„è¯„ä¼°æŠ¥å‘Šï¼Œç›´è§‚æ¯”è¾ƒä¸åŒAPIçš„æ€§èƒ½

### ğŸš€ å¿«é€Ÿå¼€å§‹

#### å®‰è£…

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/Gary-666/LLMBench.git
cd LLMBench

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

#### é…ç½®

åˆ›å»º`.env`æ–‡ä»¶å¹¶æ·»åŠ æ‚¨çš„APIå¯†é’¥å’Œæ¨¡å‹é…ç½®ï¼š

```
# APIå¯†é’¥
MONICA_KEY=your_monica_api_key
OPENAI_API_KEY=your_openai_api_key
GEMINI_KEY=your_gemini_api_key

# æ¨¡å‹é…ç½®
MONICA_MODEL=gpt-4o
OPENAI_TEXT_MODEL=gpt-4.1
OPENAI_VISION_MODEL=gpt-4.1
GEMINI_TEXT_MODEL=gemini-2.5-pro
GEMINI_VISION_MODEL=gemini-2.5-pro

# ä»£ç†é…ç½®ï¼ˆå¯é€‰ï¼‰
HTTP_PROXY=http://your-proxy:port
HTTPS_PROXY=http://your-proxy:port
```

### ğŸ“Š ä½¿ç”¨æ–¹æ³•

#### äº¤äº’å¼å¯¹è¯

ä¸ç‰¹å®šAPIè¿›è¡Œäº¤äº’å¼å¯¹è¯ï¼š

```bash
python main_new.py interactive --api monica
```

#### å¤šè½®å¯¹è¯æµ‹è¯•

ä½¿ç”¨é¢„å®šä¹‰çš„å¯¹è¯æµç¨‹æµ‹è¯•æ‰€æœ‰APIï¼š

```bash
python main_new.py multi-turn --file test_scenarios/comprehensive_test.json
```

#### æ¯”è¾ƒAPIå“åº”

æ¯”è¾ƒä¸åŒAPIå¯¹åŒä¸€æç¤ºçš„å“åº”ï¼š

```bash
python main_new.py compare --prompt "è§£é‡Šé‡å­è®¡ç®—çš„åŸºæœ¬åŸç†"
```

#### æ‰¹é‡æµ‹è¯•

è¿è¡Œä¸€ç³»åˆ—æµ‹è¯•ç”¨ä¾‹ï¼š

```bash
python main_new.py batch --file test_scenarios/basic_test.json
```

### ğŸ“ é¡¹ç›®ç»“æ„

```
LLMBench/
â”œâ”€â”€ aims/                  # æ ¸å¿ƒä»£ç ç›®å½•
â”‚   â”œâ”€â”€ cli.py             # å‘½ä»¤è¡Œæ¥å£
â”‚   â”œâ”€â”€ clients.py         # APIå®¢æˆ·ç«¯å®ç°
â”‚   â”œâ”€â”€ evaluation.py      # è¯„ä¼°ç³»ç»Ÿ
â”‚   â”œâ”€â”€ models.py          # æ•°æ®æ¨¡å‹
â”‚   â””â”€â”€ testing.py         # æµ‹è¯•å·¥å…·
â”œâ”€â”€ conversations/         # å¯¹è¯å†å²å­˜å‚¨
â”œâ”€â”€ evaluations/           # è¯„ä¼°ç»“æœå­˜å‚¨
â”œâ”€â”€ results/               # æµ‹è¯•ç»“æœå­˜å‚¨
â”œâ”€â”€ test_scenarios/        # æµ‹è¯•åœºæ™¯JSONæ–‡ä»¶
â””â”€â”€ main_new.py            # ä¸»ç¨‹åºå…¥å£
```

### ğŸ” è¯„ä¼°ç»´åº¦

AIBenchä½¿ç”¨ä»¥ä¸‹ç»´åº¦è¯„ä¼°AIå“åº”è´¨é‡ï¼š

| ç»´åº¦ | æè¿° | æƒé‡ |
|------|------|------|
| ç›¸å…³æ€§ | å›ç­”ä¸é—®é¢˜çš„ç›¸å…³ç¨‹åº¦ | 25% |
| å‡†ç¡®æ€§ | å›ç­”çš„å‡†ç¡®æ€§å’Œæ­£ç¡®æ€§ | 25% |
| å®Œæ•´æ€§ | å›ç­”çš„å…¨é¢æ€§å’Œè¯¦å°½ç¨‹åº¦ | 20% |
| è¿è´¯æ€§ | å›ç­”çš„é€»è¾‘æ€§å’Œæµç•…åº¦ | 15% |
| åˆ›æ–°æ€§ | å›ç­”çš„åˆ›æ–°æ€§å’Œç‹¬ç‰¹æ€§ | 15% |

### ğŸ§ª æµ‹è¯•åœºæ™¯

AIBenchæä¾›å¤šç§é¢„è®¾æµ‹è¯•åœºæ™¯ï¼ŒåŒ…æ‹¬ï¼š

- **åŸºç¡€é—®ç­”**: æµ‹è¯•åŸºæœ¬çŸ¥è¯†å’Œå›ç­”èƒ½åŠ›
- **ä¸Šä¸‹æ–‡è®°å¿†**: æµ‹è¯•è®°ä½ä¸Šä¸‹æ–‡ä¿¡æ¯çš„èƒ½åŠ›
- **ä¸­æ–‡ç†è§£**: æµ‹è¯•å¯¹ä¸­æ–‡è¯­è¨€å’Œæ–‡åŒ–çš„ç†è§£
- **é€»è¾‘æ¨ç†**: æµ‹è¯•é€»è¾‘æ¨ç†å’Œé—®é¢˜è§£å†³èƒ½åŠ›
- **åˆ›æ„ç”Ÿæˆ**: æµ‹è¯•åˆ›é€ åŠ›å’Œæƒ³è±¡åŠ›
- **å¤šè½®æŒ‡ä»¤**: æµ‹è¯•æ‰§è¡Œå¤æ‚æŒ‡ä»¤çš„èƒ½åŠ›

### ğŸ“ˆ è‡ªå®šä¹‰æµ‹è¯•

åˆ›å»ºè‡ªå®šä¹‰æµ‹è¯•åœºæ™¯å¾ˆç®€å•ï¼Œåªéœ€æŒ‰ä»¥ä¸‹æ ¼å¼åˆ›å»ºJSONæ–‡ä»¶ï¼š

```json
[
  {
    "name": "æµ‹è¯•åç§°",
    "description": "æµ‹è¯•æè¿°",
    "turns": [
      {"prompt": "ç¬¬ä¸€è½®é—®é¢˜"},
      {"prompt": "ç¬¬äºŒè½®é—®é¢˜"}
    ]
  }
]
```

### ğŸ“ å‘½ä»¤è¡Œå‚æ•°

| å‚æ•° | ç®€å†™ | æè¿° |
|------|------|------|
| `mode` | | è¿è¡Œæ¨¡å¼: `interactive`, `compare`, `batch`, `multi-turn` |
| `--api` | `-a` | æŒ‡å®šAPI: `monica`, `openai`, `gemini`, `all` |
| `--prompt` | `-p` | æ–‡æœ¬æç¤º |
| `--image` | `-i` | å›¾åƒURL |
| `--file` | `-f` | æµ‹è¯•åœºæ™¯JSONæ–‡ä»¶ |
| `--proxy` | | è®¾ç½®ä»£ç† |

### ğŸ“‹ è¯„ä¼°æŠ¥å‘Šç¤ºä¾‹

```
=== Overall API Scores ===
Monica API: 0.87 (æœ€ä½³)
OpenAI API: 0.85
Gemini API: 0.82

=== Dimension Breakdown ===
ç›¸å…³æ€§: Monica (0.92), OpenAI (0.89), Gemini (0.85)
å‡†ç¡®æ€§: OpenAI (0.90), Monica (0.88), Gemini (0.86)
å®Œæ•´æ€§: Monica (0.89), OpenAI (0.87), Gemini (0.82)
è¿è´¯æ€§: Monica (0.85), OpenAI (0.84), Gemini (0.80)
åˆ›æ–°æ€§: OpenAI (0.75), Monica (0.74), Gemini (0.70)
```

### ğŸ¤ è´¡çŒ®

æ¬¢è¿è´¡çŒ®ä»£ç ã€æŠ¥å‘Šé—®é¢˜æˆ–æå‡ºæ”¹è¿›å»ºè®®ï¼è¯·æŸ¥çœ‹[è´¡çŒ®æŒ‡å—](CONTRIBUTING.md)äº†è§£æ›´å¤šä¿¡æ¯ã€‚

### ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨MITè®¸å¯è¯ - è¯¦æƒ…è¯·å‚é˜…[LICENSE](LICENSE)æ–‡ä»¶ã€‚

### ğŸ“ è”ç³»æ–¹å¼

å¦‚æœ‰ä»»ä½•é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼è”ç³»æˆ‘ä»¬ï¼š

- é‚®ç®±: your.email@example.com
- GitHub: [https://github.com/Gary-666/LLMBench](https://github.com/Gary-666/LLMBench)

---

<p align="center">Made with â¤ï¸ for the AI community</p>